{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDSR_Iris.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6slZYaIsE6nM/vQHh6o80",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/artiumb/EDSR_iris/blob/master/EDSR_Iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0sDJqlrFeVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# based on https://github.com/Saafke/EDSR_Tensorflow\n",
        "#  paper: http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz1z0BwfFvEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_utils.py \n",
        "import pathlib\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "def getpathsx(path):\n",
        "    \"\"\"\n",
        "    Get all image paths from folder 'path'.\n",
        "    \"\"\"\n",
        "    data = pathlib.Path(path)\n",
        "    all_image_paths = list(data.glob('*'))\n",
        "    all_image_paths = [str(p) for p in all_image_paths]\n",
        "    return all_image_paths\n",
        "\n",
        "def getpaths(path):\n",
        "    \"\"\"\n",
        "    Get all image paths from folder 'path' while avoiding ._ files.\n",
        "    \"\"\"\n",
        "    im_paths = []\n",
        "    for fil in os.listdir(path):\n",
        "            if '.png' in fil:\n",
        "                if \"._\" in fil:\n",
        "                    #avoid dot underscore\n",
        "                    pass\n",
        "                else:\n",
        "                    im_paths.append(os.path.join(path, fil))\n",
        "    return im_paths\n",
        "\n",
        "def make_val_dataset(paths, scale, mean):\n",
        "    \"\"\"\n",
        "    Python generator-style dataset for the validation set. Creates input and ground truth.\n",
        "    \"\"\"\n",
        "    for p in paths:\n",
        "        # normalize\n",
        "        im_norm = cv2.imread(p.decode(), 3).astype(np.float32) - mean\n",
        "\n",
        "        # divisible by scale - create low-res\n",
        "        hr = im_norm[0:(im_norm.shape[0] - (im_norm.shape[0] % scale)),\n",
        "                  0:(im_norm.shape[1] - (im_norm.shape[1] % scale)), :]\n",
        "        lr = cv2.resize(hr, (int(hr.shape[1] / scale), int(hr.shape[0] / scale)),\n",
        "                        interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        yield lr, hr\n",
        "\n",
        "def make_dataset(paths, scale, mean):\n",
        "    \"\"\"\n",
        "    Python generator-style dataset. Creates 48x48 low-res and corresponding high-res patches.\n",
        "    \"\"\"\n",
        "    size_lr = 48\n",
        "    size_hr = size_lr * scale\n",
        "\n",
        "    for p in paths:\n",
        "        # normalize\n",
        "        im_norm = cv2.imread(p.decode(), 3).astype(np.float32) - mean\n",
        "\n",
        "        # random flip\n",
        "        r = random.randint(-1, 2)\n",
        "        if not r == 2:\n",
        "            im_norm = cv2.flip(im_norm, r)\n",
        "\n",
        "        # divisible by scale - create low-res\n",
        "        hr = im_norm[0:(im_norm.shape[0] - (im_norm.shape[0] % scale)),\n",
        "                  0:(im_norm.shape[1] - (im_norm.shape[1] % scale)), :]\n",
        "        lr = cv2.resize(hr, (int(hr.shape[1] / scale), int(hr.shape[0] / scale)),\n",
        "                        interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        numx = int(lr.shape[0] / size_lr)\n",
        "        numy = int(lr.shape[1] / size_lr)\n",
        "\n",
        "        for i in range(0, numx):\n",
        "            startx = i * size_lr\n",
        "            endx = (i * size_lr) + size_lr\n",
        "\n",
        "            startx_hr = i * size_hr\n",
        "            endx_hr = (i * size_hr) + size_hr\n",
        "\n",
        "            for j in range(0, numy):\n",
        "                starty = j * size_lr\n",
        "                endy = (j * size_lr) + size_lr\n",
        "                starty_hr = j * size_hr\n",
        "                endy_hr = (j * size_hr) + size_hr\n",
        "\n",
        "                crop_lr = lr[startx:endx, starty:endy]\n",
        "                crop_hr = hr[startx_hr:endx_hr, starty_hr:endy_hr]\n",
        "\n",
        "                x = crop_lr.reshape((size_lr, size_lr, 3))\n",
        "                y = crop_hr.reshape((size_hr, size_hr, 3))\n",
        "\n",
        "                yield x, y\n",
        "\n",
        "def calcmean(imageFolder, bgr):\n",
        "    \"\"\"\n",
        "    Calculates the mean of a dataset.\n",
        "    \"\"\"\n",
        "    paths = getpaths(imageFolder)\n",
        "\n",
        "    total_mean = [0, 0, 0]\n",
        "    im_counter = 0\n",
        "\n",
        "    for p in paths:\n",
        "\n",
        "        image = np.asarray(Image.open(p))\n",
        "\n",
        "        mean_rgb = np.mean(image, axis=(0, 1), dtype=np.float64)\n",
        "\n",
        "        if im_counter % 50 == 0:\n",
        "            print(\"Total mean: {} | current mean: {}\".format(total_mean, mean_rgb))\n",
        "\n",
        "        total_mean += mean_rgb\n",
        "        im_counter += 1\n",
        "\n",
        "    total_mean /= im_counter\n",
        "\n",
        "    # rgb to bgr\n",
        "    if bgr is True:\n",
        "        total_mean = total_mean[...,::-1]\n",
        "\n",
        "    return total_mean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SXkxmskF3Ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# edsr.py\n",
        "from __future__ import print_function\n",
        "\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class Edsr:\n",
        "\n",
        "    def __init__(self, B, F, scale):\n",
        "        self.B = B\n",
        "        self.F = F\n",
        "        self.scale = scale\n",
        "        self.global_step = tf.placeholder(tf.int32, shape=[], name=\"global_step\")\n",
        "        self.scaling_factor = 0.1\n",
        "        self.bias_initializer = tf.constant_initializer(value=0.0)\n",
        "        self.PS = 3 * (scale*scale) #channels x scale^2\n",
        "        self.xavier = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "        # -- Filters & Biases --\n",
        "        self.resFilters = list()\n",
        "        self.resBiases = list()\n",
        "\n",
        "        for i in range(0, B*2):\n",
        "            self.resFilters.append( tf.get_variable(\"resFilter%d\" % (i), shape=[3,3,F,F], initializer=self.xavier))\n",
        "            self.resBiases.append(tf.get_variable(name=\"resBias%d\" % (i), shape=[F], initializer=self.bias_initializer))\n",
        "\n",
        "        self.filter_one = tf.get_variable(\"resFilter_one\", shape=[3,3,3,F], initializer=self.xavier)\n",
        "        self.filter_two = tf.get_variable(\"resFilter_two\", shape=[3,3,F,F], initializer=self.xavier)\n",
        "        self.filter_three = tf.get_variable(\"resFilter_three\", shape=[3,3,F,self.PS], initializer=self.xavier)\n",
        "\n",
        "        self.bias_one = tf.get_variable(shape=[F], initializer=self.bias_initializer, name=\"BiasOne\")\n",
        "        self.bias_two = tf.get_variable(shape=[F], initializer=self.bias_initializer, name=\"BiasTwo\")\n",
        "        self.bias_three = tf.get_variable(shape=[self.PS], initializer=self.bias_initializer, name=\"BiasThree\")\n",
        "\n",
        "\n",
        "    def model(self, x, y, lr):\n",
        "        \"\"\"\n",
        "        Implementation of EDSR: https://arxiv.org/abs/1707.02921.\n",
        "        \"\"\"\n",
        "\n",
        "        # -- Model architecture --\n",
        "\n",
        "        # first conv\n",
        "        x = tf.nn.conv2d(x, filter=self.filter_one, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        x = x + self.bias_one\n",
        "        out1 = tf.identity(x)\n",
        "\n",
        "        # all residual blocks\n",
        "        for i in range(self.B):\n",
        "            x = self.resBlock(x, (i*2))\n",
        "\n",
        "        # last conv\n",
        "        x = tf.nn.conv2d(x, filter=self.filter_two, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        x = x + self.bias_two\n",
        "        x = x + out1\n",
        "\n",
        "        # upsample via sub-pixel, equivalent to depth to space\n",
        "        x = tf.nn.conv2d(x, filter=self.filter_three, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        x = x + self.bias_three\n",
        "        out = tf.nn.depth_to_space(x, self.scale, data_format='NHWC', name=\"NHWC_output\")\n",
        "        \n",
        "        # -- --\n",
        "\n",
        "        # some outputs\n",
        "        out_nchw = tf.transpose(out, [0, 3, 1, 2], name=\"NCHW_output\")\n",
        "        psnr = tf.image.psnr(out, y, max_val=255.0)\n",
        "        loss = tf.losses.absolute_difference(out, y) #L1\n",
        "        ssim = tf.image.ssim(out, y, max_val=255.0)\n",
        "        \n",
        "        # (decaying) learning rate\n",
        "        lr = tf.train.exponential_decay(lr,\n",
        "                                        self.global_step,\n",
        "                                        decay_steps=15000,\n",
        "                                        decay_rate=0.95,\n",
        "                                        staircase=True)\n",
        "        # gradient clipping\n",
        "        optimizer = tf.train.AdamOptimizer(lr)\n",
        "        gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return out, loss, train_op, psnr, ssim, lr\n",
        "\n",
        "    def resBlock(self, inpt, f_nr):\n",
        "        x = tf.nn.conv2d(inpt, filter=self.resFilters[f_nr], strides=[1, 1, 1, 1], padding='SAME')\n",
        "        x = x + self.resBiases[f_nr]\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = tf.nn.conv2d(x, filter=self.resFilters[f_nr+1], strides=[1, 1, 1, 1], padding='SAME')\n",
        "        x = x + self.resBiases[f_nr+1]\n",
        "        x = x * self.scaling_factor\n",
        "\n",
        "        return inpt + x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWF0J6TJF-6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main.py \n",
        "import tensorflow as tf\n",
        "import data_utils\n",
        "import run\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import numpy\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #gets rid of avx/fma warning\n",
        "\n",
        "# TODO:\n",
        "# When starting training for x3 and x4, start from pre-trained x2 model.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # bools\n",
        "    parser.add_argument('--train', help='Train the model', action=\"store_true\")\n",
        "    parser.add_argument('--test', help='Run PSNR test on an image', action=\"store_true\")\n",
        "    parser.add_argument('--upscale', help='Upscale an image with desired scale', action=\"store_true\")\n",
        "    parser.add_argument('--export', help='Export the model as .pb', action=\"store_true\")\n",
        "    parser.add_argument('--fromscratch', help='Load previous model for training',action=\"store_false\")\n",
        "\n",
        "    # numbers\n",
        "    parser.add_argument('--quant', type=int, help='Quantize to shrink .pb file size. 1=round_weights. 2=quantize_weights. 3=round_weights&quantize.', default=0)\n",
        "    parser.add_argument('--B', type=int, help='Number of resBlocks', default=32)\n",
        "    parser.add_argument('--F', type=int, help='Number of filters', default=256)\n",
        "    parser.add_argument('--scale', type=int, help='Scaling factor of the model', default=2)\n",
        "    parser.add_argument('--batch', type=int, help='Batch size of the training', default=16)\n",
        "    parser.add_argument('--epochs', type=int, help='Number of epochs during training', default=20)\n",
        "    parser.add_argument('--lr', type=float, help='Learning_rate', default=0.0001)\n",
        "\n",
        "    # paths\n",
        "    parser.add_argument('--image', help='Specify test image', default=\"./images/original.png\")\n",
        "    parser.add_argument('--traindir', help='Path to train images')\n",
        "    parser.add_argument('--validdir', help='Path to train images')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # INIT\n",
        "    scale = args.scale\n",
        "    meanbgr = [103.1545782, 111.561547, 114.35629928]\n",
        "\n",
        "    # Set checkpoint paths for different scales and models\n",
        "    ckpt_path = \"\"\n",
        "    if scale == 2:\n",
        "        ckpt_path = \"./CKPT_dir/x2/\"\n",
        "    elif scale == 3:\n",
        "        ckpt_path = \"./CKPT_dir/x3/\"\n",
        "    elif scale == 4:\n",
        "        ckpt_path = \"./CKPT_dir/x4/\"\n",
        "    else:\n",
        "        print(\"No checkpoint directory. Choose scale 2, 3 or 4. Or add checkpoint directory for this scale.\")\n",
        "        exit()\n",
        "\n",
        "    # Set gpu\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "\n",
        "    # Create run instance\n",
        "    run = run.run(config, ckpt_path, scale, args.batch, args.epochs, args.B, args.F, args.lr, args.fromscratch, meanbgr)\n",
        "\n",
        "    if args.train:\n",
        "        run.train(args.traindir, args.validdir)\n",
        "\n",
        "    if args.test:\n",
        "        run.testFromPb(args.image)\n",
        "        #run.test(args.image)\n",
        "    \n",
        "    if args.upscale:\n",
        "        run.upscaleFromPb(args.image)\n",
        "        #run.upscale(args.image)\n",
        "\n",
        "    if args.export:\n",
        "        run.export(args.quant)\n",
        "\n",
        "    print(\"I ran successfully.\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7VwFqdYGFhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import data_utils\n",
        "from skimage import io\n",
        "import edsr\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.python.tools import freeze_graph\n",
        "from tensorflow.python.tools import optimize_for_inference_lib\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "\n",
        "class run:\n",
        "    def __init__(self, config, ckpt_path, scale, batch, epochs, B, F, lr, load_flag, meanBGR):\n",
        "        self.config = config\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.scale = scale\n",
        "        self.batch = batch\n",
        "        self.epochs = epochs\n",
        "        self.B = B\n",
        "        self.F = F\n",
        "        self.lr = lr\n",
        "        self.load_flag = load_flag\n",
        "        self.mean = meanBGR\n",
        "\n",
        "    def train(self, imagefolder, validfolder):\n",
        "\n",
        "        # Create training dataset\n",
        "        train_image_paths = data_utils.getpaths(imagefolder)\n",
        "        train_dataset = tf.data.Dataset.from_generator(generator=data_utils.make_dataset,\n",
        "                                                 output_types=(tf.float32, tf.float32),\n",
        "                                                 output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, None, 3])),\n",
        "                                                 args=[train_image_paths, self.scale, self.mean])\n",
        "        train_dataset = train_dataset.padded_batch(self.batch, padded_shapes=([None, None, 3],[None, None, 3]))\n",
        "\n",
        "        # Create validation dataset\n",
        "        val_image_paths = data_utils.getpaths(validfolder)\n",
        "        val_dataset = tf.data.Dataset.from_generator(generator=data_utils.make_val_dataset,\n",
        "                                                 output_types=(tf.float32, tf.float32),\n",
        "                                                 output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, None, 3])),\n",
        "                                                 args=[val_image_paths, self.scale, self.mean])\n",
        "        val_dataset = val_dataset.padded_batch(1, padded_shapes=([None, None, 3],[None, None, 3]))\n",
        "\n",
        "        # Make the iterator and its initializers\n",
        "        train_val_iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
        "        train_initializer = train_val_iterator.make_initializer(train_dataset)\n",
        "        val_initializer = train_val_iterator.make_initializer(val_dataset)\n",
        "\n",
        "        handle = tf.placeholder(tf.string, shape=[])\n",
        "        iterator = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types, train_dataset.output_shapes)\n",
        "        LR, HR = iterator.get_next()\n",
        "\n",
        "        # Edsr model\n",
        "        print(\"\\nRunning EDSR.\")\n",
        "        edsrObj = edsr.Edsr(self.B, self.F, self.scale)\n",
        "        out, loss, train_op, psnr, ssim, lr = edsrObj.model(x=LR, y=HR, lr=self.lr)\n",
        "\n",
        "        # -- Training session\n",
        "        with tf.Session(config=self.config) as sess:\n",
        "\n",
        "            train_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            saver = tf.train.Saver()\n",
        "\n",
        "            # Create check points directory if not existed, and load previous model if specified.\n",
        "            if not os.path.exists(self.ckpt_path):\n",
        "                os.makedirs(self.ckpt_path)\n",
        "            else:\n",
        "                if os.path.isfile(self.ckpt_path + \"edsr_ckpt\" + \".meta\"):\n",
        "                    if self.load_flag:\n",
        "                        saver.restore(sess, tf.train.latest_checkpoint(self.ckpt_path))\n",
        "                        print(\"\\nLoaded checkpoint.\")\n",
        "                    if not self.load_flag:\n",
        "                        print(\"No checkpoint loaded. Training from scratch.\")\n",
        "                # else:\n",
        "                #     if os.path.isfile(\"./CKPT_dir/x2/\" + \"edsr_ckpt\" + \".meta\"):\n",
        "                #         saver.restore(sess, tf.train.latest_checkpoint(\"./CKPT_dir/x2/\"))\n",
        "                #         print(\"Previous checkpoint does not exists. Will load model from x2\")\n",
        "                #     else:\n",
        "                #         print(\"No checkpoint loaded. Training from scratch.\")\n",
        "\n",
        "            global_step = 0\n",
        "            tf.convert_to_tensor(global_step)\n",
        "\n",
        "            train_val_handle = sess.run(train_val_iterator.string_handle())\n",
        "\n",
        "            print(\"Training...\")\n",
        "            for e in range(1, self.epochs+1):\n",
        "\n",
        "                sess.run(train_initializer)\n",
        "                step, train_loss = 0, 0\n",
        "\n",
        "                try:\n",
        "                    while True:\n",
        "                        o, l, t, l_rate = sess.run([out, loss, train_op, lr], feed_dict={handle:train_val_handle,\n",
        "                                                                                         edsrObj.global_step: global_step})\n",
        "                        train_loss += l\n",
        "                        step += 1\n",
        "                        global_step += 1\n",
        "\n",
        "                        if step % 1000 == 0:\n",
        "                            save_path = saver.save(sess, self.ckpt_path + \"edsr_ckpt\")\n",
        "                            print(\"Step nr: [{}/{}] - Loss: {:.5f} - Lr: {:.7f}\".format(step, \"?\", float(train_loss/step), l_rate))\n",
        "\n",
        "                except tf.errors.OutOfRangeError:\n",
        "                    pass\n",
        "\n",
        "                # Perform end-of-epoch calculations here.\n",
        "                sess.run(val_initializer)\n",
        "                tot_val_psnr, tot_val_ssim, val_im_cntr = 0, 0, 0\n",
        "                try:\n",
        "                    while True:\n",
        "                        val_psnr, val_ssim = sess.run([psnr, ssim], feed_dict={handle:train_val_handle})\n",
        "\n",
        "                        tot_val_psnr += val_psnr[0]\n",
        "                        tot_val_ssim += val_ssim[0]\n",
        "                        val_im_cntr += 1\n",
        "\n",
        "                except tf.errors.OutOfRangeError:\n",
        "                    pass\n",
        "\n",
        "                print(\"Epoch nr: [{}/{}]  - Loss: {:.5f} - val PSNR: {:.3f} - val SSIM: {:.3f}\\n\".format(e,\n",
        "                                                                                                         self.epochs,\n",
        "                                                                                                         float(train_loss/step),\n",
        "                                                                                                         (tot_val_psnr / val_im_cntr),\n",
        "                                                                                                         (tot_val_ssim / val_im_cntr)))\n",
        "                save_path = saver.save(sess, self.ckpt_path + \"edsr_ckpt\")\n",
        "\n",
        "            print(\"Training finished.\")\n",
        "            train_writer.close()\n",
        "\n",
        "    def upscale(self, path):\n",
        "        \"\"\"\n",
        "        Upscales an image via model. This loads a checkpoint, not a .pb file.\n",
        "        \"\"\"\n",
        "        fullimg = cv2.imread(path, 3)\n",
        "\n",
        "        floatimg = fullimg.astype(np.float32) - self.mean\n",
        "\n",
        "        LR_input_ = floatimg.reshape(1, floatimg.shape[0], floatimg.shape[1], 3)\n",
        "\n",
        "        with tf.Session(config=self.config) as sess:\n",
        "            print(\"\\nUpscale image by a factor of {}:\\n\".format(self.scale))\n",
        "            # load the model\n",
        "            ckpt_name = self.ckpt_path + \"edsr_ckpt\" + \".meta\"\n",
        "            saver = tf.train.import_meta_graph(ckpt_name)\n",
        "            saver.restore(sess, tf.train.latest_checkpoint(self.ckpt_path))\n",
        "            graph_def = sess.graph\n",
        "            LR_tensor = graph_def.get_tensor_by_name(\"IteratorGetNext:0\")\n",
        "            HR_tensor = graph_def.get_tensor_by_name(\"NHWC_output:0\")\n",
        "\n",
        "            output = sess.run(HR_tensor, feed_dict={LR_tensor: LR_input_})\n",
        "\n",
        "            Y = output[0]\n",
        "            HR_image = (Y + self.mean).clip(min=0, max=255)\n",
        "            HR_image = (HR_image).astype(np.uint8)\n",
        "\n",
        "            bicubic_image = cv2.resize(fullimg, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            cv2.imshow('Original image', fullimg)\n",
        "            cv2.imshow('EDSR upscaled image', HR_image)\n",
        "            cv2.imshow('Bicubic upscaled image', bicubic_image)\n",
        "            cv2.waitKey(0)\n",
        "\n",
        "        sess.close()\n",
        "\n",
        "    def test(self, path):\n",
        "        \"\"\"\n",
        "        Test single image and calculate psnr. This loads a checkpoint, not a .pb file.\n",
        "        \"\"\"\n",
        "        fullimg = cv2.imread(path, 3)\n",
        "        width = fullimg.shape[0]\n",
        "        height = fullimg.shape[1]\n",
        "\n",
        "        cropped = fullimg[0:(width - (width % self.scale)), 0:(height - (height % self.scale)), :]\n",
        "        img = cv2.resize(cropped, None, fx=1. / self.scale, fy=1. / self.scale, interpolation=cv2.INTER_CUBIC)\n",
        "        floatimg = img.astype(np.float32) - self.mean\n",
        "\n",
        "        LR_input_ = floatimg.reshape(1, floatimg.shape[0], floatimg.shape[1], 3)\n",
        "\n",
        "        with tf.Session(config=self.config) as sess:\n",
        "            print(\"\\nTest model with psnr:\\n\")\n",
        "            # load the model\n",
        "            ckpt_name = self.ckpt_path + \"edsr_ckpt\" + \".meta\"\n",
        "            saver = tf.train.import_meta_graph(ckpt_name)\n",
        "            saver.restore(sess, tf.train.latest_checkpoint(self.ckpt_path))\n",
        "            graph_def = sess.graph\n",
        "            LR_tensor = graph_def.get_tensor_by_name(\"IteratorGetNext:0\")\n",
        "            HR_tensor = graph_def.get_tensor_by_name(\"NHWC_output:0\")\n",
        "\n",
        "            output = sess.run(HR_tensor, feed_dict={LR_tensor: LR_input_})\n",
        "\n",
        "            Y = output[0]\n",
        "            HR_image = (Y + self.mean).clip(min=0, max=255)\n",
        "            HR_image = (HR_image).astype(np.uint8)\n",
        "\n",
        "            bicubic_image = cv2.resize(img, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            print(np.amax(Y), np.amax(LR_input_))\n",
        "            print(\"PSNR of  EDSR   upscaled image: {}\".format(self.psnr(cropped, HR_image)))\n",
        "            print(\"PSNR of bicubic upscaled image: {}\".format(self.psnr(cropped, bicubic_image)))\n",
        "\n",
        "            cv2.imshow('Original image', fullimg)\n",
        "            cv2.imshow('EDSR upscaled image', HR_image)\n",
        "            cv2.imshow('Bicubic upscaled image', bicubic_image)\n",
        "\n",
        "            cv2.imwrite(\"./images/EdsrOutput.png\", HR_image)\n",
        "            cv2.imwrite(\"./images/BicubicOutput.png\", bicubic_image)\n",
        "            cv2.imwrite(\"./images/original.png\", fullimg)\n",
        "            cv2.imwrite(\"./images/input.png\", img)\n",
        "\n",
        "            cv2.waitKey(0)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "        sess.close()\n",
        "\n",
        "    def load_pb(self, path_to_pb):\n",
        "        with tf.gfile.GFile(path_to_pb, \"rb\") as f:\n",
        "            graph_def = tf.GraphDef()\n",
        "            graph_def.ParseFromString(f.read())\n",
        "        with tf.Graph().as_default() as graph:\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "            return graph\n",
        "\n",
        "    def testFromPb(self, path):\n",
        "        \"\"\"\n",
        "        Test single image and calculate psnr. This loads a .pb file.\n",
        "        \"\"\"\n",
        "        # Read model\n",
        "        pbPath = \"./models/EDSR_x{}.pb\".format(self.scale)\n",
        "\n",
        "        # Get graph\n",
        "        graph = self.load_pb(pbPath)\n",
        "\n",
        "        fullimg = cv2.imread(path, 3)\n",
        "        width = fullimg.shape[0]\n",
        "        height = fullimg.shape[1]\n",
        "\n",
        "        cropped = fullimg[0:(width - (width % self.scale)), 0:(height - (height % self.scale)), :]\n",
        "        img = cv2.resize(cropped, None, fx=1. / self.scale, fy=1. / self.scale, interpolation=cv2.INTER_CUBIC)\n",
        "        floatimg = img.astype(np.float32) - self.mean\n",
        "\n",
        "        LR_input_ = floatimg.reshape(1, floatimg.shape[0], floatimg.shape[1], 3)\n",
        "\n",
        "        LR_tensor = graph.get_tensor_by_name(\"IteratorGetNext:0\")\n",
        "        HR_tensor = graph.get_tensor_by_name(\"NHWC_output:0\")\n",
        "\n",
        "        with tf.Session(graph=graph) as sess:\n",
        "            print(\"Loading pb...\")\n",
        "            output = sess.run(HR_tensor, feed_dict={LR_tensor: LR_input_})\n",
        "            Y = output[0]\n",
        "            HR_image = (Y + self.mean).clip(min=0, max=255)\n",
        "            HR_image = (HR_image).astype(np.uint8)\n",
        "\n",
        "            bicubic_image = cv2.resize(img, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            print(np.amax(Y), np.amax(LR_input_))\n",
        "            print(\"PSNR of  EDSR   upscaled image: {}\".format(self.psnr(cropped, HR_image)))\n",
        "            print(\"PSNR of bicubic upscaled image: {}\".format(self.psnr(cropped, bicubic_image)))\n",
        "\n",
        "            cv2.imshow('Original image', fullimg)\n",
        "            cv2.imshow('EDSR upscaled image', HR_image)\n",
        "            cv2.imshow('Bicubic upscaled image', bicubic_image)\n",
        "\n",
        "            cv2.imwrite(\"./images/EdsrOutput.png\", HR_image)\n",
        "            cv2.imwrite(\"./images/BicubicOutput.png\", bicubic_image)\n",
        "            cv2.imwrite(\"./images/original.png\", fullimg)\n",
        "            cv2.imwrite(\"./images/input.png\", img)\n",
        "\n",
        "            cv2.waitKey(0)\n",
        "            cv2.destroyAllWindows()\n",
        "            print(\"Done.\")\n",
        "\n",
        "        sess.close()\n",
        "\n",
        "    def upscaleFromPb(self, path):\n",
        "        \"\"\"\n",
        "        Upscale single image by desired model. This loads a .pb file.\n",
        "        \"\"\"\n",
        "        # Read model\n",
        "        pbPath = \"./models/EDSR_x{}.pb\".format(self.scale)\n",
        "\n",
        "        # Get graph\n",
        "        graph = self.load_pb(pbPath)\n",
        "\n",
        "        fullimg = cv2.imread(path, 3)\n",
        "        floatimg = fullimg.astype(np.float32) - self.mean\n",
        "        LR_input_ = floatimg.reshape(1, floatimg.shape[0], floatimg.shape[1], 3)\n",
        "\n",
        "        LR_tensor = graph.get_tensor_by_name(\"IteratorGetNext:0\")\n",
        "        HR_tensor = graph.get_tensor_by_name(\"NHWC_output:0\")\n",
        "\n",
        "        with tf.Session(graph=graph) as sess:\n",
        "            print(\"Loading pb...\")\n",
        "            output = sess.run(HR_tensor, feed_dict={LR_tensor: LR_input_})\n",
        "            Y = output[0]\n",
        "            HR_image = (Y + self.mean).clip(min=0, max=255)\n",
        "            HR_image = (HR_image).astype(np.uint8)\n",
        "\n",
        "            bicubic_image = cv2.resize(fullimg, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            cv2.imshow('Original image', fullimg)\n",
        "            cv2.imshow('EDSR upscaled image', HR_image)\n",
        "            cv2.imshow('Bicubic upscaled image', bicubic_image)\n",
        "\n",
        "            cv2.waitKey(0)\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "        sess.close()\n",
        "\n",
        "    def export(self, quant):\n",
        "        print(\"Exporting model...\")\n",
        "\n",
        "        export_dir = \"./models/\"\n",
        "        if not os.path.exists(export_dir):\n",
        "                os.makedirs(export_dir)\n",
        "\n",
        "        export_file = \"EDSRorig_x{}.pb\".format(self.scale)\n",
        "\n",
        "        graph = tf.get_default_graph()\n",
        "        with graph.as_default():\n",
        "            with tf.Session(config=self.config) as sess:\n",
        "\n",
        "                ### Restore checkpoint\n",
        "                ckpt_name = self.ckpt_path + \"edsr_ckpt\" + \".meta\"\n",
        "                saver = tf.train.import_meta_graph(ckpt_name)\n",
        "                saver.restore(sess, tf.train.latest_checkpoint(self.ckpt_path))\n",
        "\n",
        "                # Return a serialized GraphDef representation of this graph\n",
        "                graph_def = sess.graph.as_graph_def()\n",
        "\n",
        "                # All variables to constants\n",
        "                graph_def = tf.graph_util.convert_variables_to_constants(sess, graph_def, ['NCHW_output'])\n",
        "\n",
        "                # Optimize for inference\n",
        "                graph_def = optimize_for_inference_lib.optimize_for_inference(graph_def, [\"IteratorGetNext\"],\n",
        "                                                                            [\"NCHW_output\"],  # [\"NHWC_output\"],\n",
        "                                                                            tf.float32.as_datatype_enum)\n",
        "                \n",
        "                # Implement certain file shrinking transforms. 2 is recommended.\n",
        "                transforms = [\"sort_by_execution_order\"]\n",
        "                if quant == 1:\n",
        "                    print(\"Rounding weights for export.\")\n",
        "                    transforms = [\"sort_by_execution_order\", \"round_weights\"]\n",
        "                    export_file = \"EDSR_x{}_q1.pb\".format(self.scale)\n",
        "                if quant == 2:\n",
        "                    print(\"Quantizing for export.\")\n",
        "                    transforms = [\"sort_by_execution_order\", \"quantize_weights\"]\n",
        "                    export_file = \"EDSR_x{}.pb\".format(self.scale)\n",
        "                if quant == 3:\n",
        "                    print(\"Round weights and quantizing for export.\")\n",
        "                    transforms = [\"sort_by_execution_order\", \"round_weights\", \"quantize_weights\"]\n",
        "                    export_file = \"EDSR_x{}_q3.pb\".format(self.scale)\n",
        "\n",
        "                graph_def = TransformGraph(graph_def, [\"IteratorGetNext\"],\n",
        "                                                      [\"NCHW_output\"],\n",
        "                                                      transforms)\n",
        "                \n",
        "                print(\"Exported file = {}\".format(export_dir+export_file))\n",
        "                with tf.gfile.GFile(export_dir + export_file, 'wb') as f:\n",
        "                    f.write(graph_def.SerializeToString())\n",
        "\n",
        "                tf.train.write_graph(graph_def, \".\", 'train.pbtxt')\n",
        "\n",
        "        sess.close()\n",
        "\n",
        "    def psnr(self, img1, img2):\n",
        "        mse = np.mean( (img1 - img2) ** 2 )\n",
        "        if mse == 0:\n",
        "            return 100\n",
        "        PIXEL_MAX = 255.0\n",
        "        return (20 * math.log10(PIXEL_MAX / math.sqrt(mse)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKuhXY5FxYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}